---
title: 항해 플러스 AI 후기) 8주차 클라우드 환경 배포 및 파인튜닝(4)
description: >-
  항해 플러스 AI 과정을 진행하면서 8주차에 학습한 내용을 정리한다.
author: 김가은
date: 2025-05-24 20:00:00 +0900
categories: [항해플러스AI]
tags: [항해99, 항해 플러스 AI 후기, AI 개발자, LLM, 딥러닝, 머신러닝, 인공지능, 자연어 처리]
pin: true
---

이번주에는 LLM을 경량화하는 여러 가지 방법을 학습하였다.

우리가 LLM을 개발하는 환경이 충분하다면 사이즈가 큰 모델도 사용할 수 있을 것이다.

하지만, 일반적으로 제한된 환경에서 모델을 사용하게 되고, 특히 LLM을 개발하는데 필요한 GPU는 가격이 매우 비싸다.

따라서 모델을 경량화하는 여러 가지 방법을 통해 제한된 환경에서도 모델을 개발할 수 있다.

## 8주차 학습한 내용: LLM 경량화

### Data Parallelism
Data Parallelism은 GPU가 여러 대 있고 각각의 GPU에 모델은 올라가지만 원하는 크기의 batch size로 학습할 수 없을 때 사용한다.

방법은 다음과 같다.

동일한 모델을 여러 GPU에 복사하여 배치하고, batch도 GPU 개수만큼 나누어 학습한다.
그리고 각 GPU에서 계산된 gradient들을 더해서 하나의 gradient로 만든다.

이 방법은 각 gradient를 합한 값이 전체 gradient와 같기 때문에 가능하다.

이 방법은 동일한 모델을 모든 GPU에 복사하기 때문에 메모리가 낭비되고, 모델이 GPU에 올라가지 않으면 사용할 수 없다는 한계가 있다.
또, out of memory가 발생하는 가장 큰 원인이 optimizer인데, optimizer state를 분배하지 않는다.

### DeepSpeed
DeepSpeed는 microsoft에서 만든, Transformer 기반의 모델들을 경량화하기 위해 만들어진 모델이다.

특히 모델 사이즈가 크고, GPU가 여러 개 있을 때 어떻게하면 메모리를 효율적으로 쓸 수 있는지를 다루고 있다.

예를 들어, zero redundancy optimizer(ZeRO)는 모델의 파라미터를 분배하여 메모리를 효율적으로 사용할 수 있도록 한다.

### Quantization
Quantization은 사용하는 bit 수를 줄여 모델을 경량화하는 방법이다.

bit수를 줄이면 정확도가 떨어지기 때문에 성능도 줄어든다.

16-bit, 8-bit, 4-bit 등이 있고, A100 GPU에서만 사용할 수 있는 BFloat16 형식도 있다.

**Automatic mixed precision(AMP)**
bit수를 줄이면 성능도 줄어든다. 이를 보완하기 위한 방법이 AMP이다.

AMP는 낮은 정확도의 floating point만 사용하는 것이 아니라 기존의 높은 정확도의 floating point도 섞어서 사용하는 방법이다.

이렇게 floating point를 섞어서 사용하면 정확도가 높아지고, 메모리를 효율적으로 사용할 수 있다.

### Parameter-efficient fine-tuning(PEFT)
마지막 layer 등 일부 layer만 학습하여 메모리를 효율적으로 사용하는 방법이다.
일부 layer만 학습하는 경우, 학습하는 layer의 parameter만 구하면 되므로 메모리가 줄어든다.

**Low-rank adaptation(LoRA)**
LoRA는 모델의 각 레이어에 작은 크기의 rank decomposition matrix를 추가하여 모델을 경량화하는 방법이다.

LoRA를 사용해도 성능은 크게 줄어들지 않기 때문에 LLM 경량화를 위한 방법으로 많이 사용된다.

targetModel을 지정하는 것에 따라 LoRA를 적용하는 레이어가 달라진다.
예를 들어, torch.nn.Linear를 사용하면 Transformer의 대부분의 레이어가 Linear이므로 거의 전체 레이어를 LoRA로 학습하게 된다.

반면에 q_proj, k_proj, v_proj, o_proj를 targetModel로 지정하면 나머지 레이어는 학습하지 않겠다는 것이므로 메모리는 더 적게 사용하게 된다.

> 멘토링 시간에 LoRA를 사용하면서 주의할 점에 대해 질문하였다.
> 코치님께서는 LoRA 자체의 문제보다는 Multi LoRA로 서빙할 때 속도가 느려지는 문제를 경험하신 적이 있다고 한다.

### Flash Attention
Flash attention은 Transformer의 attention 계산을 최적화하는 방법이다.
다른 방법과 달리 성능의 저하와 같은 트레이드오프가 없다.

## 과제

이번 주 과제는 경량화를 적용해보는 과제였다.

### 기본과제
기본과제는 LoRA의 rank를 조절하여 성능을 비교하는 과제였다.

LoRA rank를 8, 128, 256로 설정하여 비교하였다.

LoRA의 rank를 증가시킬수록 학습성능이 향상되고, runtime은 더 증가하는 것을 확인하였다.

128과 256은 train loss와 eval loss가 비슷하게 나온다.

rank에 따른 학습 성능의 차이를 고려하면 LoRA rank는 128로 설정하는 것이 적절해 보인다.

LoRA 코드는 복잡하지 않다.

```python
  # LoRA 설정
  lora_config = LoraConfig(
      # LoRA rank
      r=8,
      # LoRA alpha
      # LoRA alpha는 학습 속도를 조절하는 파라미터
      # 값이 클수록 학습 속도가 빨라짐
      lora_alpha=32,
      # LoRA 적용 레이어
      target_modules=["q_proj", "v_proj"],
      ...
  )

  # LoRA 적용
  model = get_peft_model(model, lora_config)
```

### 심화과제

심화과제는 현재 진행하고 있는 최종 프로젝트에 경량화를 적용해보는 것이다.

현재 나는 최종 프로젝트에 gpt api만 적용하고 Closed LLM 모델은 적용하지 않은 상태였다.

SFT를 적용하고 모델을 경량화해보기로 했다.

먼저 SFT 학습을 위한 데이터를 생성하였다.

gpt와 claude로 데이터를 생성해봤는데, claude가 더 잘 생성해주었다.

```js
  {
    "instruction": "불안한 직장인에게 위로와 심리적 안정감을 주는 책 3권을 추천해줘. 각 책마다 추천 이유와 감정적 효과를 설명해줘.",
    "input": "감정: 불안함\n직업: 직장인\n효과: 위로, 안정\n상황: 잠들기 전 휴식\n집중도: 가볍게 읽을 수 있음",
    "output": "1. 제대로 연습하는 법 : 어학부터 스포츠까지, 인지심리학이 제시하는 배움의 기술 - 아투로 E. 허낸데즈 지음 ; 방진이 옮김\n   - 추천 이유: 불안하고 자신감이 부족할 때, 심리학 기반의 학습법이 내면의 안정감을 줍니다.\n   - 이 책이 도움이 될 수 있는 이유: 작은 습관 변화로 긍정적 변화를 얻고, 불안을 줄일 수 있습니다."
  },
```

그리고 colab 환경에서 모델을 학습시켰다.

colab 무료 환경에서는 모델을 학습시키기 어려웠다.

처음에는 동일한 모델에 대해서 LoRA를 적용하기 전/후를 비교하고 싶었지만 out of memory로 학습이 잘 진행되지 않았다.

그래서 방법을 바꿔서, LoRA를 적용하기 전에는 사용하지 못했던 모델을 LoRA를 적용해서 학습시키는 것으로 진행하였다.

LoRA를 적용하기 전에는 gpt2-large 모델부터 out of memory로 학습이 진행되지 않았다.

LoRA를 적용하면 gemma-2b-it 모델까지 학습을 진행할 수 있었다.

gpt2-lg, gpt2-xl, facebook/opt-1.3b, gemma-2b-it 모델을 시도해봤는데, 
테스트한 모델 중 가장 사이즈가 큰 gemma-2b-it는 다른 모델들과 다르게 loss가 높게 나왔다.

학습 데이터가 많지 않고, 동일한 LoRA 코드를 사용했기 때문에 학습률, batch size 등 모델별 최적화가 필요한 것으로 보인다. 

loss만으로 성능을 판단하기는 어려우며, 성능에 대한 추가 검토가 필요해 보인다.

------------------------

이렇게 마지막 주차가 끝이 났다.

한주 한주 과제를 제출하는 것만 해도 쉽지 않았는데 마지막까지 어떻게든 제출은 한 것 같다.

제출만 하고 아쉬운 과제도 많았지만, 제출했다는 것 자체가 뿌듯하기도 하다.

다음주에는 수료식이 진행된다.
