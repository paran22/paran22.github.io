---
title: 항해 플러스 AI 후기) 2주차 딥러닝 이론과 자연어 처리
description: >-
  항해 플러스 AI 과정을 진행하면서 2주차에 학습한 내용을 정리한다.
author: 김가은
date: 2025-04-03 23:00:00 +0900
categories: [항해플러스AI]
tags: [항해99, 항해 플러스 AI 후기, AI 개발자, LLM, 딥러닝, 머신러닝, 인공지능]
pin: true
---

이번주에는 자연어를 처리하는 딥러닝 모델에 대해 다루었다.

지난주 멘토링 시간에 코치님이 다음주에는 자연어라서 좀 더 재미있을거라고 하셨는데........... 🫠

이번주에 새로 배운 개념과 이론은 정말 어려웠다.

그렇지만 함께 AI 코스를 듣고있는 동료들과, 학습메이트와, 코치님과 함께 공부하면서 많은 부분을 새롭게 배울 수 있었고, 그런 면에서 정말 재미있는 한 주 였다. ⭐️

2주차에 학습한 내용을 정리하고, 스스로 어떤 성장을 했는지 회고해보았다.

## 2주차 학습한 내용: 자연어 처리와 Transformer 모델

### 1. 자연어 처리와 자연어 처리에 필요한 기술
인간의 언어로 이루어진 입력을 처리하는 문제이다.
감정 분석(주어진 문장이 부정인지 긍정인지), 질의 응답, 기계 번역, 요약 등의 문제를 해결할 수 있다.
자연어는  이미지와 달리 가변적인 길이를 가지고 있고, 순서와 맥락이 중요하기 때문에 MLP를 사용해서 처리하기 어렵다.

#### Tokenizer
자연어를 딥러닝 모델로 처리하기 위해서는 텍스트를 처리하기 쉬운 작은 단위인 토큰으로 분리한다.
예를 들어, 단어 기반으로 토큰을 분리하면 다음과 같이 분리할 수 있다.
```
"나는 학교에 갑니다" → ["나는", "학교에", "갑니다"]
```
이를 모델이 이해할 수 있도록 Vocabulary 라는 사전을 통해 숫자로 변환한다.
```
["나는", "학교에", "갑니다"] -> [2, 35, 398]
```

다양한 tokenizer library를 통해 tokenizer를 구현할 수 있다.

tokenizer에 따라 문장의 시작을 의미하는 `[CLS]`, 문장의 끝을 의미하는 `[SEP]` 등의 특수 토큰이 추가될 수 있다.

**Padding token**
서로 다른 길이의 입력 시퀀스를 동일한 길이로 맞추기 위해 사용하는 특수 토큰이다.
보통 0으로 표현하며, `<PAD>` 또는 `[PAD]`와 같은 특수 토큰으로 지정한다.
패딩 토큰이 모델의 학습에 영향을 주지 않도록 마스킹 처리를 한다.

#### Word Embedding
벡터로 만드는 행위를 embedding이라고 한다.
Word Embedding은 단어를 벡터 공간에 표현하는 방법이다.
비슷한 의미를 가진 단어들은 벡터 공간에서 서로 가깝게 위치한다.

embedding 초기에는 렌덤한 값을 가진다.
3만 차원의 one-hot vector(3만개 중에 1개만 1, 나머지는 0인 벡터)로 모든 단어간의 거리가 동일한 상태이다. 
여기에 임베딩 레이어의 가중치를 곱해서 784차원 임베딩 벡터로 만든다.
즉, 학습을 하는 과정에서 비슷한 의미를 갖는 단어는 벡터공간 상에서도 가까워지도록 학습이 되는 것이다. 

### 2. Recurrent Neural Network(RNN)
MLP로 처리하지 못하는 텍스트 데이터를 처리하기 위해 등장한 모델이다.
Hidden state를 통해 앞에서 등장한 토큰의 정보를 기억하기 때문에 단어 순서에 따라 달라지는 의미 차이를 학습하고 표현할 수 있다.
시간복잡도는 O(n), 공간복잡도는 O(1)이다.

##### 기울기 소실(Vanishing Gradient) 문제
순차적으로 정보가 전달되는 과정에서 시퀀스가 길어질수록 초기에 입력된 정보가 점차 소실되는 기울기 소실(Vanishing Gradient) 문제가 발생한다.
이를 해결하기 위해 LSTM 등 여러 모델이 등장하였고, Transformer도 이 문제를 해결하기 위해 등장하였다.

##### 장점
과거 데이터는 점점 잊어버리고 최신 데이터를 중요하게 인식하기 때문에 주식, 생체신호, 건강 상태와 같이 최신 데이터가 중요한 문제에 적합하다.

#### Sequence to Sequence
기계 번역이나 질의 응답과 같이 입력과 출력이 모두 text인 경우 RNN 하나로 해결할 수 없다.
이를 해결하기 위해 인코더와 디코더 모두 RNN을 사용하는 Sequence to Sequence 모델을 사용한다.

#### Teacher forcing
학습할 때 생성한 token을 다음 input token으로 사용할 경우 잘못된 token을 생성했을 때 오답이 누적되어 최종 생성되는 문장이 잘못될 수 있다.
이를 방지하기 위해 input token이 아니라 실제 label을 사용해서 학습한다.

### 3. Attention
Transformer 모델의 핵심 구성 요소이며, BERT, GPT 등 현대 언어 모델의 기반이다.
##### 기울기 소실 문제 해결
RNN은 기울기 소실 문제때문에 긴 문장의 정보를 처리하기 어렵다.
Attention은 모든 입력 토큰들 간의 관계를 계산하여 기울기 소실 문제를 해결한다.
어떤 토큰이 다른 토큰들과 어떤 관계가 있는지 파악하여 문장의 의미를 더 정확하게 이해하고, attention 해야하는 부분에 더 집중할 수 있도록 한다.

##### 병렬 처리
병렬 처리가 가능하여 계산 효율을 높인다.
Query(Q), Key(K), Value(V) 행렬의 연산을 통해 계산된다.

#### Softmax
Attention 메커니즘에서 각 단어의 중요도를 계산하기 위해 attention score를 사용한다.
이 값들을 그대로 사용할 경우, 값의 범위가 너무 크거나 작아서 학습이 어려울 수 있다.

이를 해결하기 위해 softmax 함수를 사용하여 attention score를 확률 분포로 변환한다. softmax 함수는 입력 벡터의 값을 0과 1 사이의 값으로 변환하며, 모든 값의 합이 1이 되도록 한다.
이렇게 변환된 값은 각 단어의 중요도를 확률적으로 해석할 수 있게 해준다.

예를 들어, attention score가 [2.0, 1.0, 0.1]이라면, softmax 함수를 적용하여 [0.7, 0.2, 0.1]로 변환된다. 
이 값들은 각 단어가 문맥에서 차지하는 중요도의 비율을 나타내며, 합이 1이 된다. 
이를 통해 모델은 각 단어의 중요도를 더 명확하게 이해하고, 학습 과정에서 안정적으로 사용할 수 있다.

#### Self-Attention
같은 문장 내에서 단어들 간의 관계를 파악한다.
예를 들어, "나는 학교에서 점심을 먹었다"라는 문장이 있으면 각 단어가 문장 내 모든 단어와의 관련성을 계산한다.
```
# "점심을"이라는 단어의 attention 점수 예시
attention_scores = {
    "나는": 0.1,      # 관련성 낮음
    "학교에서": 0.2,   # 관련성 중간
    "점심을": 0.3,    # 자기 자신
    "먹었다": 0.4     # 관련성 높음 (점심을 '먹었다'의 관계)
}
```
이를 통해 "점심을"이라는 단어가 "먹었다"와 가장 강한 관계를 가졌다는 것을 알 수 있다.
이를 통해 문장의 의미를 더 정확하게 이해한다.

문장이 길어도 모든 단어 쌍의 관계를 직접 계산할 수 있고, 문맥에 기반하여 단어의 의미를 파악할 수 있다.

#### Multi-Head Attention
Attention matrix는 렌덤하게 초기화되기 때문에 처음부터 문장을 왜곡하여 학습할 수 있다.
Multi-Head Attention은 여러 개의 Attention head를 사용하여 왜곡된 학습을 방지하고, 다양한 관점에서 문장을 이해한다.
각 head는 다른 중요한 패턴을 학습하며, 최종적으로 결과를 결합하기 때문에 표현력이 상승한다.
각 Attention head는 독립적으로 병렬 연산이 가능하며, 이를 통해 연산 속도를 크게 향상시킬 수 있다.

### 4. Transformer
RNN을 제거하고 순수하게 attention 메커니즘을 사용하는 모델이다.
**Attention is all you need** 논문에서 소개되었다.
시간복잡도는 O(1), 공간복잡도는 O(n^2)이다.
RNN에 비해 상대적으로 메모리를 많이 사용하기 때문에, 메모리 문제를 개선하기 위한 방법들도 제시되었다.
#### Positional encoding
Transformer는 입력 시퀀스의 순서 정보를 포함시키기 위해 Positional encoding을 사용한다.
각 위치마다 고유한 패턴을 생성하고 sin과 cos 함수를 사용하여 상대적 위치 정보를 표현한다.
#### Feed-forward layer
Feed-Forward Network는 MLP의 한 종류로, attention이 찾은 관계를 더 깊게 처리하고, 특징 추출을 강화하는 역할을 한다.
2개의 선형 레이어로 구성되어 있으며, ReLU 활성화 함수를 사용하여 비선형성을 추가한다.

### 5. 모델의 성능 파악: Loss와 Accuracy의 차이
딥러닝에서 모델이 학습을 한다는 것은 손실함수가 최소화되는 방향으로 모델의 가중치(파라미터)가 업데이트되는 것이다.
손실함수가 최소화되는 방향으로 업데이트하기 위해서는 가중치에 대한 미분이 가능해야 한다.
Loss는 모델이 얼마나 틀렸는지를 측정하는 것이다.
Loss는 미분이 가능하기 때문에 기울기를 바탕으로 loss가 작아지는 방향으로 업데이트 할 수 있다.

반면에 Accuracy는 미분이 불가능하기 때문에 모델을 학습할 수 없다.
대신 모델이 얼마나 잘 맞히는지를 설명할 수 있다.

모델의 성능을 평가하는 evaluation metric(평가 지표)은 아주 많다.
evaluation metric은 수백만개 이상의 데이터에 대해 정량적으로 성능을 평가할 수 있는 지표이다.
어떠한 문제를 해결하기 위해 모델을 만들 때 모델이 개선되고 있는지를 확인하기 위해서는 적절한 평가 지표를 설정하는 것이 매우 중요하다.

## 어려웠던 점과 이를 해결한 방법
이번주에는 발제 자료에서 다루는 이론을 이해하는게 가장 어려웠다.
기존의 모델들이 자연어 처리에 적합하지 않다는 것은 이해했지만, 그 대안으로 제시된 Attetion과 Transformer 모델이 어떤 식으로 구성되고 동작하는지 이해하는데 어려움이 있었다.

그렇지만 발제가 끝나고 함께 모여서 공부하고, 같이 자료를 찾아본 것이 많은 도움이 되었다.
다같이 유튜브를 찾아보고, "저게 저 뜻이었어...!" 라고 함께 고개를 끄덕이기도 하고, 막히는 건 학습메이트님들이나, 코치님에게 물어보면서 해결해나갔다.

개인적으로 학습할 때 큰 그림을 그리는게 중요하다고 생각하는데, 혼자서 공부했다면 절대 이해하지 못했을 것 같다.

다른 분들이 질문하는 걸 들으면서 새롭게 궁금한 점이 생겨서 질문하기도 하고, 찾아보기도 하면서 조금씩 사고가 확장되어 가는 느낌이 좋았다.👍

## 과제
이번주 과제는 Transformer 모델을 직접 구현해보는 과제였다.
모델을 직접 구현할 수 있을 정도로 이해할 수 있다면 좋았겠지만, 이번주 목표는 큰 그림을 잡는 것이었기 때문에 모델 구현은 ai의 도움을 많이 받았다.
대신 코드에 대해 ai에게 질문하고, 주석을 작성하면서 어떤 코드인지 이해하려고 노력했다.
수학적인 부분은 아직 많이 어렵고, 모델을 제대로 구성했는지 확신하지 못하지만 에러 없이 모델을 학습시켜 제출한 것에 의의를 두기로 했다.
다른 사람들의 도움이 없었다면 아에 제출하지 못했을 것 같다.
함께 공부한다는 것의 중요성과 즐거움을 많이 느낀 한 주였다!
