---
title: 항해 플러스 AI 후기) 1주차 머신러닝과 딥러닝의 핵심 개념
description: >-
  항해 플러스 AI 과정을 진행하면서 1주차에 학습한 내용을 정리한다.
author: 김가은
date: 2025-03-28 23:00:00 +0900
categories: [항해플러스AI]
tags: [항해99, 항해 플러스 AI 후기, AI 개발자, LLM, 딥러닝, 머신러닝, 인공지능]
pin: true
---

항해 플러스 AI 3기를 시작하게 되었다.
https://hanghae99.spartacodingclub.kr/plus/ai
AI 커리큘럼을 보면서 따라갈 수 있을지 고민하다가 결국 마감 직전에 신청하게 되었다. 🤣

나에게 AI는 굉장히 낯선 분야이다.
매주 학습하면서 배우는 내용들을 정리하기 위해 이 글을 작성한다.

## 항해 플러스 AI 사전과정

항해 플러스에서는 온보딩을 위한 사전과정이 준비되어 있다.
그런데 합류를 늦게 하는 바람에 사전과정에 참여할 수 있는 시간이 며칠밖에 되지 않았다.

항해 플러스 AI 과정을 잘 따라갈 수 있도록 파이썬과 머신러닝, 딥러닝 등 관련 강의들이 사전 제공된다.

파이썬은 그래도 조금 사용해봐서 크게 걱정되지는 않았는데 머신러닝과 딥러닝은 완전히 처음 접하는 분야라서 걱정이 많았다.

개강이 며칠 안남은 상황에서 온보딩 매니저님은 시간이 부족하다면 우선 딥러닝 기본 강의를 듣는 걸 추천해주셨다.

그래서 이틀동안 빡세게 강의를 들었다.🏃

강의를 들으면서 모르는 개념의 연속이었지만 그래도 딥러닝이 무엇이고 코드로 어떻게 동작하는지 볼 수 있었다.


## 항해 플러스 AI 과정 1주차 정기 모임

드디어 개강날이 되었다.

항해 플러스는 매주 토요일에 정규 모임이 진행된다.

학습 과정에 따라서 온라인으로 진행하기도 하는 것 같은데, AI 과정은 매주 오프라인으로 진행된다.

항해플러스 AI, 프론트엔드, 백엔드가 모두 같은 날 시작해서 다같이 공통 OT를 진행했다.

![6A32F61B-43FC-41AB-BE54-920348B2678B_1_105_c](https://github.com/user-attachments/assets/0155c52e-3306-4a13-a82d-b38284fc7d76)

사람이 정~말 많았다.

기본적으로 매주 발제와 과제가 진행이 되고, 매주 팀별로 멘토링도 진행된다.

AI 과정끼리도 OT를 진행하고, 팀원들과 인사도 했다.

기본적으로 모두 개인 과제이지만 팀원들과 온라인 공간에서 같이 공부하고 멘토링도 진행한다.

## 1주차 학습한 내용: 머신러닝과 딥러닝의 핵심 개념

1주차 주제는 머신러닝과 딥러닝의 핵심 개념이다.

발제를 들으면서 사전과정을 안들었으면 정말 하나도 이해를 못했을 것이라는 생각이 가장 먼저 들었다.

발제는 많이 어려웠지만 그래도 사전 강의를 들으면서 들어본 용어가 있어서 따라갈 수 있었던 것 같다.

이번 주에 학습한 주요 내용은 다음과 같다.

### 인공지능 > 머신러닝 > 딥러닝
인공지능이 가장 넓은 개념이고, 머신러닝은 인공지능의 하위분야, 딥러닝은 머신러닝의 하위분야이다.

**1. 인공지능 (AI, Artificial Intelligence)**
  - 가장 큰 개념으로, 인간의 지능을 모방하여 문제를 해결하는 기술
  - 체스 게임, 음성비서 (시리, 빅스비), 자율주행 자동차

**2. 머신러닝 (Machine Learning)**
  - AI의 하위 분야로, 데이터를 통해 학습하여 패턴을 찾아내는 기술
  - 사람이 직접 특성을 설계하고 선택한다(Feature Extraction)
  - 스팸 메일 분류, 넷플릭스 영화 추천 시스템, 날씨 예측

**3. 딥러닝 (Deep Learning)**
  - 머신러닝의 한 종류로, 인간의 뇌 구조를 모방한 인공신경망을 사용
  - 얼굴 인식 시스템, 이미지 분류, GPT와 같은 대규모 언어 모델

| 구분 | 머신러닝 (Machine Learning) | 딥러닝 (Deep Learning) |
| --- | --- | --- |
| 학습 방식 | 사람이 직접 특성(Feature Extraction)을 설계하고 선택 | 데이터로부터 자동으로 특성 학습 |
| 데이터 요구량 | 상대적으로 적은 데이터로도 학습 가능 | 대량의 데이터 필요 |
| 연산 능력 | 비교적 적은 컴퓨팅 파워 필요 | 높은 컴퓨팅 파워 필요 |
| 모델 구조 | 비교적 단순한 알고리즘 사용 | 복잡한 신경망 구조 사용 |
| 적용 분야 | 선형적이거나 비교적 단순한 패턴 인식에 적합 <br> 정형 데이터 분석, 예측, 분류 | 매우 복잡하고 비선형적인 패턴도 학습 가능 <br> 이미지/음성 인식, 자연어 처리 |
| 장점 | 해석이 용이하고 구현이 상대적으로 간단 | 복잡한 패턴 인식 능력이 뛰어남 |
| 단점 | 복잡한 패턴 학습에 한계가 있음 | 학습에 많은 시간과 자원이 필요 |

### 머신러닝의 주요 요소
**1. Task**

- 머신러닝에서 해결하고자 하는 문제의 유형을 의미한다.
- 이메일 스팸 분류, 주택 가격 예측, 질병 진단 등

**2. Evaluation Metric (평가 지표)**

- 모델의 성능을 측정하는 기준이다.
- 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), 평균제곱오차(MSE) 등

**3. Optimization (최적화)**

- 모델의 성능을 향상시키기 위해 파라미터를 조정하는 과정이다.
- Gradient Descent(손실 함수를 최소화하기 위해 기울기를 따라 파라미터를 조정하는 방법)

### Linear Regression
**1. Linear regression 모델**

- 입력 변수(x)와 출력 변수(y) 사이의 선형 관계를 예측하는 모델이다.
- 예시
    1. 집값 예측
        - 입력: 면적, 방 개수, 위치 등
        - 출력: 집값
    2. 판매량 예측
        - 입력: 광고비, 계절, 경쟁사 가격 등
        - 출력: 예상 판매량
    3. 학생 성적 예측
        - 입력: 공부 시간, 출석률, 과제 제출 등
        - 출력: 시험 점수
- 수학적으로 표현하면 어떤 x가 들어왔을 때 ($x^{(1)}$ or $x^{(2)}$)를 예측하는 함수를 구현하는 것이다. 여기서 $w_1, w_2, b$는 AI 모델 **$f$의 parameter**이다. 원하는 예측 값을 잘 출력하는 parameter를 찾는 것이 목적이다. 최적화를 통해 좋은 성능을 내는 파라미터를 찾을 수 있다.

$$
f(x; w_1, w_2, b) = w_1x^{(1)} + w_2x^{(2)} + b.
$$

**2. Gradient descent (경사 하강법)**
  - 최적화 방법 중 하나이다.
  - 현재 위치에서 가장 가파른 내리막 방향으로 이동한다.
  - 계속 반복하여 최저점(local minimum)에 도달한다.

![img](https://github.com/user-attachments/assets/6d49203c-1a53-4919-a723-72238d62ab2d)


**3. 한계**
- Linear regression는 직선 하나만을 가지기 때문에 XOR 문제는 풀 수 없다.

![스크린샷 2025-03-28 오후 10 32 35](https://github.com/user-attachments/assets/0510f39a-1761-4d57-9834-84d8b3f79568)


### Multi-Layer Perceptron(MLP)

**1. Multi-layer Perceptron(MLP)**

- linear regression 모델이 풀 수 없는 비선형 문제를 해결하기 위한 모델
- linear regression 모델에 비선형 함수 ReLU를 추가하여 비선형 데이터를 처리할 수 있다.
- linear regression 모델을 여러 개를 쌓아 훨씬 복잡한 데이터도 처리할 수 있다.
- MLP는 여러 층으로 구성된 인공신경망을 사용하고, 자동으로 특성을 학습하고 역전파로 심층 학습을 진행하는 가장 기본적인 딥러닝 모델이다.

**2. Backpropagation(역전파)**

- MLP를 깊게 쌓아 parameter들이 늘어나도 gradient를 쉽게 계산할 수 있게 고안된 방법
- 각 node들의 gradient를 계산하고 parameter와 평가 지표 사이의 gradient들을 모두 곱하여 최종 gradient를 계싼한다.
- PyTorch를 사용하면 backpropagation을 구현하여 gradient를 계산해준다.

**3. Stochastic Gradient Descent(SGD)**

- 데이터가 많아도 메모리가 부족하지 않게 gradient descent를 진행하기 위해 고안된 방법
- 데이터를 여러 개의 batch로 나누어 gradient descent 를 진행한다.

### Overfitting

**1. Overfitting**

- 딥러닝에서 사용하는 데이터는 딥러닝 모델을 학습하기 위해 평가 지표로 사용한 Train data와 학습하는 단계에서는 보지 못했던 Test data로 구분할 수 있다.
- Test data가 train data와 너무 다르면 성능이 좋지 않을 수 있다.
- 딥러닝 모델이 학습을 너무 잘해서 train data를 완전히 외워버리면 train data 외의 다른 데이터에 대한 성능은 좋지 않을 수 있다. 이를 generalization 성능이 떨어지고 overfitting이 발생했다고 한다.

**2. 해결방법**

**Validation data & early stopping**

- train data에서 추출한 validation data를 사용해 validation
- train data와 test data 사이에 validation data를 추가하고, validation data에 대한 loss(validation loss)를 계산하여 overfitting이 일어나는 지점을 확인할 수 있다.
- 이렇게 학습을 일찍 멈추는 것을 early stopping이라고 한다.

![overfit2](https://github.com/user-attachments/assets/f330bde6-ea36-49c8-9f3f-1f320a435d52)

### 딥러닝 모델의 generalization 성능을 높이는 방법

**Weight Decay**

- overfitting을 막기 위한 regularization 기법
- 모델이 완만한 곡선의 출력을 가지게 되면 generalization 성능이 좋아진다는 것에 착안하여 parameter들을 작게 만드는 term을 추가한다.
- 기존의 loss에 추가하는 term을 regularizer라고도 부른다.

**Dropout**

- 하나의 MLP에서 특정 확률로 각 node를 제외시키는 방법
- 실험 결과 generalization 성능이 좋은 것으로 드러남

**Layer Normalization**

- 딥러닝 모델들의 layer 중간에 normalization을 적용하는 하여 각 layer의 출력 분포가 비슷해지도록 한다.
- 학습이 빨라지고 generalization 성능도 올라간다.

**Adam Optimizer**

- Local minimum에 빠져서 Global minimum을 찾지 못하는 것을 방지하기 위해 이전에 움직이고 있던 방향(관성)과 극솟점과의 거리를 고려하여 learning rate를 조절한다.
- gradient descent보다 느리고 메모리를 많이 잡아먹지만, 훨씬 좋은 학습 성능을 보여주기 때문에 현대의 거의 모든 딥러닝 모델이 사용하고 있다.

![img (2)](https://github.com/user-attachments/assets/5e4b9bd8-6dce-449b-a541-a2ed3f980178)


## 이번 주에 해결한 문제

과제 진행은 쉽지 않았다.
딥러닝에서는 jupyter notebook이라는 환경에서 코드를 작성하는데, 파일을 실행시키는 것에서부터 어려웠다.

또, 과제에서는 cuda를 사용해서 GPU를 사용하는데, cuda는 MAC 환경에서는 사용할 수 없었다.
MAC에서 GPU를 사용하기 위해 mps를 사용했다.

그리고 과제를 진행하면서 정의한 모델로 학습을 시키게 되는데 시간이 정말 오래걸렸다.
이거는 멘토링을 하면서 힌트를 얻을 수 있었다.

우선 jupyter notebook을 실행할 때 Colab을 사용할 수 있다는 것을 알게되었다.
그런데 Colab에서도 동일하게 시간이 엄청 많이 걸린다는 것을 확인했다.
그래서 코드에 이상이 없는지 다시 확인했고, 모델의 정확도를 측정하기 위해 정확도를 계산해서 배열에 저장하는 코드의 위치가 잘못됬다는 것을 알게 되었다.

![KakaoTalk_Photo_2025-03-28-22-41-34](https://github.com/user-attachments/assets/ac197241-0840-425d-88ad-3220a0df291a)


한 번 학습할 때마다 정확도를 측정하면 되는데, 한번 학습할 때 테스트 데이터 당 한번씩 정확도를 계산하고 있었다.
그러니 시간이 엄청나게 걸릴 수밖에 🥲

과제를 제출하기 전에 알게 되서 다행이었다.

## 이번 주에 배운 것, 좋았던 점

AI에 대해서 굉장히 막연하게 인공지능에게 뭔가를 학습시킨다고 알고 있었는데, 실제로 코드로 구현해볼 수 있어서 좋았다.

코드를 작성하고, 실행시키면서 어떤 과정을 거쳐서 학습시키는지 알 수 있었고,
머신러닝과 딥러닝의 여러 개념들이 코드로는 어떻게 구현되는지 볼 수 있었다.

머신러닝과 딥러닝의 개념들을 잘 이해하지는 못했지만, 이제는 딥러닝 모델을 구현한다고 하면 좀 더 구체적인 이미지로 생각할 수 있게 되었다.

또, 멘토링을 하면서 내가 이 과정을 통해 얻으려는 목적이 무엇인지 다시 생각해볼 수 있었다.

나는 앞으로 AI에 대해서 스스로 학습할 수 있는 기본기를 쌓고, LLM을 활용해서 서비스를 만들고 싶어서 이 과정에 합류하였다.

딥러닝 모델을 직접 구현하는 것보다 이 기술이 어떻게 활용될 수 있는지에 좀 더 집중하고 싶다.

퇴근 후에 학습하는게 쉽지 않지만 무사히 9주 동안 과정을 잘 마치고 싶다.💪





