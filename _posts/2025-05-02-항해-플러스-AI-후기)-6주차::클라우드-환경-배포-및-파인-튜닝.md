---
title: 항해 플러스 AI 후기) 6주차 클라우드 환경 배포 및 파인튜닝
description: >-
  항해 플러스 AI 과정을 진행하면서 6주차에 학습한 내용을 정리한다.
author: 김가은
date: 2025-05-06 23:00:00 +0900
categories: [항해플러스AI]
tags: [항해99, 항해 플러스 AI 후기, AI 개발자, LLM, 딥러닝, 머신러닝, 인공지능, 자연어 처리, GPT, Streamlit, AWS, ec2, vLLM]
pin: true
---

이번 주에는 개인사정으로 토요일 오프라인 모임에 참여하지 못하였다.
발제는 녹화 영상으로 제공되지만, 오프라인에서 진행하는 과제 리뷰나, Q&A에 참여하지 못해서 아쉬웠다.

특히 과제 리뷰를 할 때, 다른 분들은 어떤 고민을 했는지 들을 수 있고 내가 생각하지 못한 다양한 시도들도 알 수 있어서 좋았다.
이번 주에는 참여하지 못해서 아쉬웠고, 남은 기간에는 꼭 빠지지 않고 참석해야겠다.

항해 플러스 참여를 고민하시는 분이 이 글을 읽으신다면 발제 영상이 제공되더라도 가능한 오프라인 모임에 꼭 참여하는 걸 ⭐️**추천**⭐️한다.

## 6주차 학습한 내용: 클라우드 환경 배포 및 파인튜닝

### LLM 배포하기

이번 주에는 본격적으로 프로젝트에 들어가면서, LLM 모델을 클라우드에 배포하는 방법에 대해 학습하였다.

항해 플러스에서는 AWS EC2를 사용해서 LLM 서버를 배포하였다.
EC2에는 다양한 인스턴스 유형을 제공하는데, 이 중 **g4dn**과 같이 GPU가 제공되는 인스턴스가 있다.

GPU가 제공되는 인스턴스를 사용한다면 CUDA를 설치해서 사용할 수 있다.
LLM 모델을 배포할 때 GPU를 사용하면 병렬 처리가 가능해서 더 빠르게 처리할 수 있다.

지난 주 과제를 진행하면서 openLLM 모델을 사용했는데 로컬에서 실행할 때는 응답 속도가 매우 느렸다.

멘토링 시간에 어떻게 하면 속도를 올릴 수 있는지 질문했는데, vLLM과 같은 환경을 구축하는 것이 필요하다는 답변을 받았다.

vLLM은 GPU에서 LLM 모델의 추론 속도를 최적화하기 위해 설계된 라이브러리이다.
vLLM은 다음과 같은 방식으로 GPU 성능을 최적화한다.

- 페이지드 어텐션(Paged Attention): vLLM의 핵심 기술로, GPU 메모리를 작은 블록으로 나누어 KV 캐시를 효율적으로 관리한다. 이를 통해 GPU 메모리 낭비를 4% 이하로 줄여 더 많은 요청을 동시에 처리할 수 있다.
- CUDA 최적화 커널: vLLM은 NVIDIA GPU용 CUDA 최적화 커널을 사용하여 GPU 성능을 극대화한다.
- 텐서 병렬화: 여러 GPU에 걸쳐 모델을 분산하여 대규모 모델도 효율적으로 처리할 수 있다.
- 연속 배치 처리: 실시간으로 들어오는 요청을 효율적으로 배치 처리하여 GPU 자원 활용도를 높인다.

LLM 모델을 배포할 때 GPU는 중요한 역할을 한다.
그렇지만 GPU의 가격이 매우 비싸기 때문에 초기에는 GPU를 사용할 수 있는 클라우드 환경을 이용하고, 점진적으로 자체적인 서버를 구축하는 것이 좋다.

또, LLM 모델을 직접 개발하는 것이 아니라 api를 사용한다면 GPU를 사용하지 않아도 된다.

현재 상황과 비용을 고려하여 환경을 구축하는 것이 중요하다.

### Streamlit

Streamlit은 웹 어플리케이션을 쉽게 만들 수 있도록 ui와 배포 등 여러가지 기능을 제공하는 라이브러리이다.

https://streamlit.io/

LLM이나 데이터 분야를 개발하면 파이썬으로 개발을 하게 되는데, Streamlit은 파이썬 라이브러리이기 때문에 시각적인 확인이 필요하거나, 프로토타입을 개발할 때 유용하게 사용할 수 있다.

특히 프론트엔드 개발이 필요하지만 프론트엔드에 대한 경험이 없는 경우에 간단한 코드로도 프론트엔드를 구현할 수 있다.

이렇게 ui를 제공하는 라이브러리의 특성상 커스텀을 할 때는 어려웠지만 프로토타입은 정말 빠르게 구현할 수 있었다.

## 과제 구현

이번 과제는 주로 ui를 개발하는 과제였다.

과제 자체는 어렵지 않았는데, 심화과제가 지난 주에 구현한 최종 프로젝트 프로토타입에 ui를 개발하는 것이라서, 최종 프로젝트 구현 방식을 고민하는 데 시간을 더 많이 사용했다.

### 기본 과제

기본과제는 간단한 챗봇을 개발하는 것이었다.
이미지를 업로드하고 gpt api를 사용해서 이에 대한 질의응답을 진행하는 기능을 구현하였다.

이전에 과제를 진행하면서 챗봇을 구현해놓은 것이 있어서 Streamlit을 사용하지 않고 기존에 만든 코드에 이미지 업로드 기능을 추가해서 구현하였다.

챗봇은 next.js의 api routes를 사용해서 api를 만들고, ui는 cursor를 사용해서 구현하였다. 

이미지를 업로드해서 gpt api를 사용하는 간단한 기능만 구현하였기 때문에 파이썬을 사용하지 않고 자바스크립트로도 충분히 구현할 수 있었지만, LLM 모델을 개발한다면 파이썬을 사용하는 것이 더 편할 것 같다.

<img width="706" alt="스크린샷 2025-05-06 오전 12 57 50" src="https://github.com/user-attachments/assets/c2dc7c5e-91da-4d87-84da-050472a3b4c9" />


### 심화 과제

이번 주 심화과제는 지난 주 심화과제에서 구현한 내용에 ui를 적용하는 것이었다.

지난 주 심화과제에서는 최종 프로젝트 주제로 고민 중이던 주제를 프로토타입으로 구현해보았다.
챗봇으로 간단한 대화를 나누고, 이를 바탕으로 도서를 추천해주는 챗봇이다.

추천할 도서 데이터는 국립중앙도서관에서 공공 api로 제공하는 사서추천도서 데이터를 사용하였다.
사서추천도서 데이터에는 책의 주요 정보와 목차, 도서를 추천하는 이유가 포함되어 있어 추천 데이터로 사용하기에 적절하다고 생각했다.

추천도서 데이터를 벡터 db에 저장하고, 벡터 db에서 사용자의 질문과 유사한 도서를 검색해 LLM으로 추천 이유를 생성하는 방식으로 구현하였다.

그런데 과제를 진행하면서 도서 추천 기능 자체는 LLM 보다는 추천 데이터와 추천 알고리즘을 어떻게 구현하는지가 중요하다고 느껴졌다.

멘토링 시간을 활용하서 고민하고 있는 부분에 대해 질문했다.
도서 데이터에 사용자가 읽은 책 목록이나, 사용자의 독서 취향과 같은 데이터를 추가해서 RAG를 여러 단계로 사용해보라는 피드백을 받았다.

좀 더 LLM을 활용할 수 있는 주제로 바꾸는 것이 좋을지 고민했는데, 멘토링 시간에 받은 피드백을 바탕으로 좀 더 여러 가지 시도를 해보았다.

우선 사용자가 읽은 책 목록과 사용자의 독서 취향 데이터를 추가하였다.

테스트를 위해 테스트 시나리오를 하나 작성해서 데이터를 만들었다.

그리고 RAG에서 이 데이터들을 좀 더 활용해서 추천 도서 후보 데이터를 선정하고, LLM을 활용해서 이 중 적합한 도서를 최종적으로 선정하는 방식으로 구현해보았다.

RAG에 처음부터 사용자 취향 정보를 반영해서 유사 도서를 검색하는 방법도 있고,
RAG에는 사용자 취향 정보 없이 질문만을 가지고 유사 도서를 선정하고 LLM을 좀 더 활용해서 이 중 사용자 취향에 적합한 도서를 선정하는 방법도 활용할 수 있을 것 같다.

LLM 모델은 우선 api를 사용해서 구현하고, 이후에 openLLM을 적용해보면 좋을 것 같다.


