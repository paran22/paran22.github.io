---
title: 항해 플러스 AI 후기) 3주차 LLM 원리와 작동 방식
description: >-
  항해 플러스 AI 과정을 진행하면서 3주차에 학습한 내용을 정리한다.
author: 김가은
date: 2025-04-11 23:00:00 +0900
categories: [항해플러스AI]
tags: [항해99, 항해 플러스 AI 후기, AI 개발자, LLM, 딥러닝, 머신러닝, 인공지능, 자연어 처리, BERT, GPT]
pin: true
---

이번주에는 언어 모델인 BERT와 GPT에 대해 학습하였다.

과제를 BERT 위주로 진행해서, 아직 GPT는 많이 낯설다.

먼저 이번주에 학습한 내용을 정리하고, 과제를 중심으로 이번 주에 어려웠던 점을 회고하였다.

## 3주차 학습한 내용: LLM 원리와 작동 방식

RNN이나 Transformer와 같은 모델들은 train data가 충분하다면 좋은 성능을 발휘하지만, train data는 항상 충분하지 않다.
data가 충분하지 않은 상황에서도 test data에서 잘 동작하는 모델을 만들기 위해 Pre-trained Model을 사용한다.

### 1. Transfer learning
다른 자연어 처리 문제를 푸는 모델을 활용하여 데이터가 적은 자연어 처리 문제를 해결할 수 있을 거라는 아이디어에서 시작한다.
목표로 하는 자연어 처리 문제를 **downstream task**라고 한다.

대표적인 pre-trained model로 BERT, GPT가 있다.

#### Pre-training
pre-trained model은 충분한 데이터로 학습한 모델을 말한다.
pre-training은 두 가지 방식으로 나눌 수 있다.
##### Supervised Pre-training
학습에 사용되는 데이터가 정답이 있는 경우이다. 즉, 입력 데이터와 그에 대응하는 정답 레이블이 쌍으로 존재한다.
예를 들어, 감정 분석(긍정/부정), 스팸 메일 분류(스팸/일반)과 같은 문제에서 사용된다.
이러한 방식은 특정 태스크에 대해 높은 정확도를 보이지만, 레이블이 있는 데이터를 구하기 어렵고 비용이 많이 든다는 단점이 있다.

##### Unsupervised Pre-training
학습에 사용되는 데이터가 정답이 없는 경우로, 대규모의 텍스트 데이터만을 사용한다.
주어진 text로부터 가상의 label을 생성하여 학습한다. 예를 들어, 문장의 다음 단어 예측하기, 마스킹된 단어 맞추기 등의 작업을 수행한다.
모델이 스스로 데이터 내의 패턴, 구조, 관계를 학습한다. 
GPT나 BERT와 같은 대규모 언어 모델들이 이 방식으로 학습되며, 인터넷에서 수집한 방대한 양의 텍스트 데이터를 활용할 수 있다는 장점이 있다.

#### Fine-tuning
pre-trained model을 가지고 목표로 하는 자연어 처리 문제를 해결하기 위해 학습시키는 방법을 말한다.
fine-tuning은 두 가지 방식으로 나눌 수 있다.

##### Parameter를 포함하여 전체 모델 학습
사전 학습된 모델의 모든 파라미터를 새로운 태스크에 맞게 업데이트하는 방식이다. 
충분한 데이터와 컴퓨팅 자원이 있을 때 사용하며, 모델 전체를 목표 태스크에 맞게 세밀하게 조정할 수 있다는 장점이 있다.

사전 학습된 모델의 모든 파라미터를 포함하여 학습하고, 학습 시에는 각 파라미터에 대한 gradient와 파라미터의 현재 상태를 메모리에 저장하기 때문에 필요한 메모리가 많다.
파라미터가 많은데 적은 데이터로 학습하면 overfitting이 발생할 수 있다.

##### Parameter를 들고 오되, 특정 layer들을 freeze하여 학습
사전 학습된 모델의 일부 층은 고정(freeze)하고 나머지 층만 학습시키는 방식이다.
일반적으로 classification layer만 학습시키고, 나머지는 고정한다.
데이터가 적거나 컴퓨팅 자원이 제한적일 때 효과적이며, 학습 시간도 단축할 수 있다.
상대적으로 메모리나 overfitting의 위험이 적다.

### 2. BERT
Bidirectional Encoder Representations from Transformers의 약자이다.
2018년에 구글에서 발표한 모델이다.

BERT는 트랜스포머(Transformer)의 인코더(Encoder) 부분만을 사용한 모델이다. 
디코더가 없어 텍스트 생성 작업에는 적합하지 않다.

self-supervised 방식으로 pre-training하는데, 이는 데이터 자체에서 자동으로 레이블을 생성하여 학습하는 방식이다. 
BERT는 주로 두 가지 태스크로 pre-training을 수행한다:

**1. Masked Language Model (MLM)**
문장의 일부 단어를 마스킹하고 이를 예측한다.
문장의 앞뒤 문맥을 모두 고려하여 단어의 의미를 파악한다.

**2. Next Sentence Prediction (NSP)**
두 문장이 이어지는 문장인지 예측한다.
두 문장이 자연스럽게 이어지는지 판단하면서 더 큰 맥락을 이해할 수 있다.

문장의 앞뒤 문맥을 동시에 고려할 수 있어 텍스트 이해와 분석에 효과적이다.
문장 분류, 개체명 인식, 질의응답과 같은 이해 중심 태스크에 적합하다.

**DistillBERT**
DistillBERT는 knowledge distillation을 통해 더 작은 모델로 학습하는 방법이다.

### 3. GPT
Generative Pre-trained Transformer의 약자이다.

GPT는 트랜스포머의 디코더(Decoder) 부분만을 사용한 모델이다. 
이전에 생성된 토큰들을 기반으로 다음 토큰을 순차적으로 생성하는 자기회귀적(Auto-regressive) 모델이다.
현재 토큰 이전의 문맥(왼쪽 문맥)만 참조할 수 있고, BERT와 달리 마스킹된 미래 정보를 볼 수 없다.
연속적이과 일관된 텍스트를 생성할 수 있어 스토리텔링, 대화, 번역 등 생성 태스크에 적합하다.

next-token prediction 방식으로 pre-training을 수행한다.
MLM과 달리 문장의 중간이 아닌 마지막 토큰만 예측하는 방식이다.

GPT-3는 대규모 데이터셋을 사용하여 학습하여 fine-tuning없이도 다양한 자연어 처리 문제를 해결할 수 있다.
어떤 자연어 처리 문제를 풀 때 그 문제의 예시들을 먼저 보여주는 few-shot learning 방식을 사용하면 더 좋은 성능을 보인다.

## 과제

이번 주의 기본 과제는 DistillBERT로 뉴스 기사 분류 모델을 학습하는 것이었다.
허깅페이스의 AG_News 데이터셋을 사용하여 모델을 학습시켰다.
https://github.com/paran22/hanghae_plus_ai_assignment/blob/main/assignment/3-basic

실습 예제가 DistillBERT이라서 실습 파일을 참고해서 구현하니 크게 어렵지는 않았다.
코드 자체도 이전주차에 transformer를 구현할 때 보다는 간단했다.
pre-trained model이라서 이전의 과제들과 달리 첫번째 epoch부터 낮은 loss를 보이는 것이 신기했다.

심화과제는 3개의 task를 선택적으로 구현하는 것이었다.
두 문장이 주어졌을 때 논리적으로 연결이 되어 있는지, 서로 모순되는지, 아니면 아예 무관한지 분류하는 MNLI를 선택하였다.

그런데 이 심화과제가 정말 쉽지 않았다.
https://github.com/paran22/hanghae_plus_ai_assignment/tree/main/assignment/3-hard
코드 자체는 cursor를 사용하고 있어서 구현이 어렵지는 않았는데, 처음에 사용한 DistillBERT의 성능이 너무 낮게 나와서 어떻게 하면 성능을 올릴 수 있을지 고민이 많았다.

cursor chat으로 계속 대화를 하면서 batch size나 learning rate를 조절하고, epochs도 변경해보았지만 크게 성능이 좋아지지 않았다.
파인튜닝하지 않은 모델의 성능이 test accuracy 0.32였는데, 최종적으로 0.3870까지밖에 올리지 못하였다.

학습을 도와주는 학습메이트님들과 다른 수강생분들과 얘기를 해보니 DistillBERT 모델의 한계로 보였다.

그래서 DistillBERT보다 모델 크기가 큰 RoBERTa를 사용해보았다.

모델 하나만 바꿨을뿐인데 RoBERTa는 처음 시도부터 test accuracy가 0.75가 나왔다.

대신 train accuracy가 급격하게 올라가서 freeze layer를 추가했더니 이번에는 학습이 너무 오래걸리고 성능도 더 낮아졌다.

gradual unfreezing을 적용하니 처음에는 학습이 잘 되지 않다가 반절 정도 freeze layer를 해제하니 급격하게 test accuracy가 올라가는 걸 볼 수 있었다.

멘토링 때 코치님은 파인튜닝에 사용하는 데이터 수도 작고, 이미 pretrained 하면서 자연어에 대해 학습한 부분을 잊어버릴 수 있다고 freeze layer를 추가하는걸 추천하셨는데, gradual unfreezing을 한 것과 freeze layer를 아에 추가하지 않은 것이 결과적으로는 test accuracy가 비슷하게 나와서 혼란스럽다.🫠

아직 과제 피드백은 나오지 않아서, 피드백보고 더 찾아보고 물어봐야 할 것 같다.

다른 분들 과제가 공개되면, 코드 참고해서 더 살펴봐야 할 것 같다.

과제를 만족스럽게 끝내지는 못했지만, 다른 주차보다 이것저것 시도해보고 고민해보는 시간이 많았던 것 같아 좋았다.



